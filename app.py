"""Streamlit app for Adult Income classification model exploration."""

from __future__ import annotations

import json
from pathlib import Path
from typing import Dict

import joblib
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import streamlit as st
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    roc_auc_score,
)


BASE_DIR = Path(__file__).resolve().parent
DATA_DIR = BASE_DIR / "data"
MODEL_DIR = BASE_DIR / "model"


@st.cache_resource
def load_metadata() -> Dict:
    """Load metadata generated by the training script."""
    metadata_path = MODEL_DIR / "metadata.json"
    with metadata_path.open("r", encoding="utf-8") as file:
        return json.load(file)


@st.cache_resource
def load_all_models() -> Dict[str, object]:
    """Load all persisted model pipelines from disk."""
    models: Dict[str, object] = {}
    for model_file in MODEL_DIR.glob("*_pipeline.joblib"):
        display_name = model_file.stem.replace("_pipeline", "").replace("_", " ").title()
        models[display_name] = joblib.load(model_file)
    return models


def compute_metrics(y_true: pd.Series, y_pred: pd.Series, y_prob: pd.Series, positive_label: str) -> Dict[str, float]:
    """Compute evaluation metrics for binary classification."""
    y_binary = (y_true == positive_label).astype(int)
    metrics = {
        "Accuracy": accuracy_score(y_true, y_pred),
        "AUC": roc_auc_score(y_binary, y_prob),
        "Precision": precision_score(y_true, y_pred, pos_label=positive_label),
        "Recall": recall_score(y_true, y_pred, pos_label=positive_label),
        "F1 Score": f1_score(y_true, y_pred, pos_label=positive_label),
        "MCC": matthews_corrcoef(y_true, y_pred),
    }
    return metrics


def render_confusion_matrix(y_true: pd.Series, y_pred: pd.Series, labels: list[str]) -> None:
    """Render confusion matrix as a heatmap."""
    matrix = confusion_matrix(y_true, y_pred, labels=labels)
    fig, ax = plt.subplots(figsize=(5, 4))
    sns.heatmap(matrix, annot=True, fmt="d", cmap="Blues", xticklabels=labels, yticklabels=labels, ax=ax)
    ax.set_xlabel("Predicted")
    ax.set_ylabel("Actual")
    ax.set_title("Confusion Matrix")
    st.pyplot(fig)


def build_manual_input(metadata: Dict) -> pd.DataFrame:
    """Create one-row feature input form in the sidebar."""
    st.sidebar.subheader("Manual Prediction Input")
    values = {}
    for column in metadata["feature_columns"]:
        default_value = metadata["default_values"][column]
        if isinstance(default_value, float) or isinstance(default_value, int):
            values[column] = st.sidebar.number_input(
                label=column,
                value=float(default_value),
            )
        else:
            values[column] = st.sidebar.text_input(
                label=column,
                value=str(default_value),
            )
    return pd.DataFrame([values])


def main() -> None:
    """Run Streamlit interface."""
    st.set_page_config(page_title="ML Classifier Dashboard", layout="wide")
    st.title("Adult Income Classification Dashboard")
    st.write("Compare models and run predictions using pre-trained pipelines.")

    if not MODEL_DIR.exists():
        st.error("Model directory not found. Run `python train_models.py` first.")
        return

    metadata = load_metadata()
    models = load_all_models()

    if not models:
        st.error("No trained models available. Run `python train_models.py` first.")
        return

    st.sidebar.header("Controls")
    model_names = sorted(models.keys())
    selected_model_name = st.sidebar.selectbox("Choose model", options=model_names)
    threshold = st.sidebar.slider("Decision threshold", min_value=0.10, max_value=0.90, value=0.50, step=0.05)

    selected_model = models[selected_model_name]
    positive_label = metadata["positive_label"]
    class_labels = metadata["class_labels"]

    st.subheader("Saved Benchmark Metrics (Test Split)")
    comparison_path = MODEL_DIR / "model_comparison.csv"
    if comparison_path.exists():
        score_df = pd.read_csv(comparison_path)
        st.dataframe(score_df, width="stretch")
    else:
        st.warning("model_comparison.csv not found.")

    st.subheader("Upload CSV for Evaluation")
    st.caption("Upload a CSV with the same feature columns. Include `income` column to compute metrics.")
    uploaded_file = st.file_uploader("Upload input CSV", type=["csv"])

    if uploaded_file is not None:
        uploaded_df = pd.read_csv(uploaded_file)
        missing_columns = set(metadata["feature_columns"]) - set(uploaded_df.columns)
        if missing_columns:
            st.error(f"Missing required feature columns: {sorted(missing_columns)}")
        else:
            features_df = uploaded_df[metadata["feature_columns"]].copy()
            probabilities = selected_model.predict_proba(features_df)[:, 1]
            predicted_labels = pd.Series(
                [positive_label if p >= threshold else class_labels[0] for p in probabilities],
                name="prediction",
            )
            result_df = uploaded_df.copy()
            result_df["predicted_income"] = predicted_labels.values
            result_df["probability_gt_50k"] = probabilities
            st.write("Predictions Preview")
            st.dataframe(result_df.head(20), width="stretch")

            if metadata["target_column"] in uploaded_df.columns:
                y_true = uploaded_df[metadata["target_column"]].astype(str).str.strip()
                metrics = compute_metrics(y_true, predicted_labels, probabilities, positive_label)
                st.write("Evaluation Metrics")
                st.dataframe(pd.DataFrame([metrics]), width="stretch")
                render_confusion_matrix(y_true, predicted_labels, class_labels)
                report = classification_report(y_true, predicted_labels, output_dict=True)
                st.write("Classification Report")
                st.dataframe(pd.DataFrame(report).transpose(), width="stretch")
            else:
                st.info("Target column `income` not found in uploaded CSV, so only predictions are shown.")

    st.subheader("Manual Single Prediction")
    manual_input_df = build_manual_input(metadata)
    if st.button("Predict Manual Input"):
        probabilities = selected_model.predict_proba(manual_input_df)[:, 1]
        label = positive_label if probabilities[0] >= threshold else class_labels[0]
        st.success(f"Predicted class: {label}")
        st.write(f"Probability of {positive_label}: {probabilities[0]:.4f}")
        st.dataframe(manual_input_df, width="stretch")

    st.subheader("Reference Test Set Evaluation")
    reference_path = DATA_DIR / "test_reference.csv"
    if reference_path.exists():
        reference_df = pd.read_csv(reference_path)
        x_ref = reference_df[metadata["feature_columns"]]
        y_ref = reference_df[metadata["target_column"]]
        y_prob = selected_model.predict_proba(x_ref)[:, 1]
        y_pred = pd.Series(
            [positive_label if p >= threshold else class_labels[0] for p in y_prob],
            name="prediction",
        )
        ref_metrics = compute_metrics(y_ref, y_pred, y_prob, positive_label)
        st.dataframe(pd.DataFrame([ref_metrics]), width="stretch")
    else:
        st.info("Reference test set not found. Run training script first.")


if __name__ == "__main__":
    main()
